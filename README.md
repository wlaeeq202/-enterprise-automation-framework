---

## ğŸ§© Enterprise Data Simulation (Snowflake-Style ETL)

This repository includes a **deterministic, CI-safe simulation of a Snowflake ETL pipeline**.

It demonstrates how an SDET validates **data correctness and performance**, not just UI or API responses.

### ğŸ“ Folder Structure

```text
enterprise-data-simulation/
â””â”€ snowflake/
   â”œâ”€ data/
   â”‚  â”œâ”€ raw_costs.json
   â”‚  â””â”€ snowflake_fact_costs.json
   â”œâ”€ scripts/
   â”‚  â”œâ”€ generate_costs.js
   â”‚  â”œâ”€ transform_costs.js
   â”‚  â””â”€ validate_transformation.js
   â”œâ”€ perf/
   â”‚  â””â”€ baseline.json
   â””â”€ README.md
```

```text
.github/workflows/etl-validate.yml
```
### ğŸ”„ What the ETL Workflow Does

1. Generate representative raw data (5,000 rows)
2. Transform raw â†’ fact output (GROUP BY + SUM)
3. Independently re-aggregate raw data
4. Compare expected vs actual results
5. Measure execution runtime
6. Fail CI if:
   - Data mismatches are detected
   - Runtime regression exceeds allowed threshold

---

## â± Runtime Regression Protection

In addition to validating data correctness, the ETL pipeline also protects
against **performance regressions**.

### How Runtime Is Evaluated

- ETL execution time is measured during CI
- A historical baseline is used when available
- Safe defaults are applied when no baseline exists

```text
enterprise-data-simulation/snowflake/perf/baseline.json
```

### Why CI Uses 5,000 Rows (and Not Millions)

HealthMonix-style pipelines can process **millions of rows**, but CI must stay fast.

This repo intentionally uses **5,000 representative rows** in PR builds to:
- Catch ETL logic regressions quickly (GROUP BY / SUM rules)
- Keep pull request pipelines fast and reliable
- Prevent deployments from slowing down due to validation suites

### How This Scales to Millions

The same scripts work for larger datasets by changing an environment variable:

- PR / CI: `ROWS=5000` (fast correctness + quick runtime signal)
- Nightly / scheduled: `ROWS=100000+` (heavier performance validation)

This is the same strategy enterprise teams use:
**fast correctness checks per commit + scheduled performance gates**

---

## â–¶ï¸ Run ETL Locally

### 1) Install dependencies
```bash
npm ci
```
```bash
npm run etl:generate
npm run etl:transform
npm run etl:validate
```
```bash
ROWS=10000 npm run etl:generate
npm run etl:transform
npm run etl:validate
```
#### macOS / Linux
```bash
$env:ROWS="10000"
npm run etl:generate
npm run etl:transform
npm run etl:validate
```
#### Windows
```powershell
$env:ROWS="10000"
npm run etl:generate
npm run etl:transform
npm run etl:validate
```



